{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.display import HTML\n",
    "from bs4 import BeautifulSoup as bs\n",
    "from pathlib import Path\n",
    "import sys\n",
    "import os\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def load_job_requirements_df(html_files_dir):        \n",
    "    # holds all job tiles and requirements listed for that title\n",
    "    job_requirements = {}\n",
    "    # run over each htm or html file and extra stuff\n",
    "    for html_file in list(Path().joinpath(html_files_dir).glob(\"*.htm*\")):\n",
    "        with open(html_file) as fp:\n",
    "            soup = bs(fp)\n",
    "            job = soup.find('title')\n",
    "            if not job:\n",
    "                job = 'unknown'\n",
    "            else:\n",
    "                job = job.text.split('-')[0].strip().lower()\n",
    "            # pull out the text in each bullet (if there is any)         \n",
    "            requirements = [x.text.strip() for x in soup.find_all('li') if len(x) > 0]\n",
    "            # if we've seen this job title before add the requirements to the existing list\n",
    "            # which may introduce duplicates but we'll reduce that later\n",
    "            if job in job_requirements.keys():\n",
    "                job_requirements[job] = requirements + job_requirements[job]\n",
    "            else:\n",
    "                job_requirements[job] = requirements\n",
    "        \n",
    "    # get a list of all the job titles\n",
    "    jobs = job_requirements.keys()\n",
    "\n",
    "    # create tuples (job, requirements) removing any duplicates\n",
    "    raw_data = [(job, list(set(job_requirements[job]))) for job in jobs if len(job_requirements[job]) > 0]\n",
    "    \n",
    "    return pd.DataFrame(raw_data, columns=['Job Title', 'List of Requirements'])\n",
    "\n",
    "    \n",
    "\"\"\"\n",
    "Assumptions: \n",
    " (1) executed from within the resume-job-posting-nlp-project cloned repo\n",
    " (2) ./data/html_job_postings.zip already unzipped\n",
    "\"\"\"\n",
    "\n",
    "html_files_dir = os.path.join(os.getcwd(), 'data', 'html_job_postings')\n",
    "\n",
    "if not os.path.exists(html_files_dir):\n",
    "    html_files_dir = input(\"Please enter the absolute path to the html files directory\")\n",
    "    if not os.path.exists(html_files_dir): sys.exit(\"Failed to locate html files folder!\")\n",
    "\n",
    "data = load_job_requirements_df(html_files_dir)    \n",
    "\n",
    "#  save to file\n",
    "pickled_file = 'data_frame_from_step_1.pkl'\n",
    "\n",
    "data.to_pickle(pickled_file)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
